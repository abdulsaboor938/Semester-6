{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_nbjj3LZ82WG"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gM6THUO886E-"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    return (np.sum((x1 - x2)**2))**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Fe_wvUnz7fgF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, training_data, training_labels):\n",
        "        self.training_data = training_data\n",
        "        self.training_labels = training_labels\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        return [self._predict(x) for x in test_data]\n",
        "\n",
        "    def _predict(self, test_sample):\n",
        "        # Compute distances between test_sample and all examples in the training set\n",
        "        distances = np.sqrt(np.sum((self.training_data - test_sample) ** 2, axis=1))\n",
        "\n",
        "        # Sort by distance and return indices of the first k neighbors\n",
        "        nearest_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "        # Extract the labels of the k nearest neighbor training samples\n",
        "        nearest_labels = self.training_labels[nearest_indices]\n",
        "\n",
        "        # Return the most common class label\n",
        "        return mode(nearest_labels)[0][0]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZjyaZ5e8759"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KhU34cpb9AT_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self._mean = np.zeros((n_classes, X.shape[1]), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, X.shape[1]), dtype=np.float64)\n",
        "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for i, j in enumerate(self._classes):\n",
        "            X_c = X[y == j]\n",
        "            self._mean[i, :] = X_c.mean(axis=0)\n",
        "            self._var[i, :] = X_c.var(axis=0)\n",
        "            self._priors[i] = X_c.shape[0] / float(X.shape[0])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(x) for x in X]\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # calculate posterior probability for each class\n",
        "        for i, j in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[i])\n",
        "            posterior = np.sum(np.log(self._pdf(i, x)))\n",
        "            posterior = prior + posterior\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        # return class with the highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "\n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        return norm.pdf(x, loc=mean, scale=np.sqrt(var))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7gQkqTp9r1s",
        "outputId": "4af7bcea-d9e9-4bd2-e734-bc3ac6121391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN classification accuracy 1.0\n",
            "Naive Bayes classification accuracy 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/5f/1b1zms0j03d6wzq402h8fw8c0000gn/T/ipykernel_36464/3307280108.py:26: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
            "  return mode(nearest_labels)[0][0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Imports\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    from sklearn import datasets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "    iris = datasets.load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=1234\n",
        "    )\n",
        "\n",
        "    k = 3\n",
        "    clf = KNN(k=k)\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    print(\"KNN classification accuracy\", accuracy(y_test, predictions))\n",
        "\n",
        "    nb = NaiveBayes()\n",
        "    nb.fit(X_train, y_train)\n",
        "    predictions = nb.predict(X_test)\n",
        "    print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of Knn is 1.0\n"
          ]
        }
      ],
      "source": [
        "#knn from kibrary\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'The accuracy of Knn is {model.score(X_test, y_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of naive BAyes is 1.0\n"
          ]
        }
      ],
      "source": [
        "# naive bayes from library\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'The accuracy of naive BAyes is {model.score(X_test, y_test)}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
