{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_nbjj3LZ82WG"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gM6THUO886E-"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2)**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fe_wvUnz7fgF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        return [self._predict(x) for x in x_test]\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = self._compute_distances(x)\n",
        "        nearest_indices = self._find_k_nearest(distances)\n",
        "        return self._classify(nearest_indices)\n",
        "\n",
        "    def _compute_distances(self, x):\n",
        "        return np.sqrt(np.sum((self.x - x) ** 2, axis=1))\n",
        "\n",
        "    def _find_k_nearest(self, distances):\n",
        "        return np.argsort(distances)[:self.k]\n",
        "\n",
        "    def _classify(self, nearest_indices):\n",
        "        nearest_labels = self.y[nearest_indices]\n",
        "        return mode(nearest_labels)[0][0]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZjyaZ5e8759"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KhU34cpb9AT_"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self._mean = np.zeros((n_classes, X.shape[1]), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, X.shape[1]), dtype=np.float64)\n",
        "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y==c]\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[idx, :] = X_c.var(axis=0)\n",
        "            self._priors[idx] = X_c.shape[0] / float(X.shape[0])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(x) for x in X]\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # calculate posterior probability for each class\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[idx])\n",
        "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
        "            posterior = prior + posterior\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        # return class with highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "        \n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7gQkqTp9r1s",
        "outputId": "4af7bcea-d9e9-4bd2-e734-bc3ac6121391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN classification accuracy 1.0\n",
            "Knn classification accuracy (library) 1.0\n",
            "Naive Bayes classification accuracy 1.0\n",
            "Naive Bayes classification accuracy (library) 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/5f/1b1zms0j03d6wzq402h8fw8c0000gn/T/ipykernel_42447/4009852673.py:28: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
            "  return mode(nearest_labels)[0][0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Imports\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    from sklearn import datasets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "        return accuracy\n",
        "\n",
        "    iris = datasets.load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=1234\n",
        "    )\n",
        "\n",
        "    k = 3\n",
        "    clf = KNN(k=k)\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    print(\"KNN classification accuracy\", accuracy(y_test, predictions))\n",
        "\n",
        "    # implementing from library\n",
        "\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "    knn.fit(X_train, y_train)\n",
        "    pred = knn.predict(X_test)\n",
        "    print('Knn classification accuracy (library)', accuracy(y_test, pred))\n",
        "\n",
        "    nb = NaiveBayes()\n",
        "    nb.fit(X_train, y_train)\n",
        "    predictions = nb.predict(X_test)\n",
        "    print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))\n",
        "\n",
        "    # implementing from library\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    gnb = GaussianNB()\n",
        "    model = gnb.fit(X_train, y_train)\n",
        "    preds = gnb.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    print(\"Naive Bayes classification accuracy (library)\", accuracy(y_test, preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
