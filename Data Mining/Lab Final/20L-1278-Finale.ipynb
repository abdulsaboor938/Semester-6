{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a notebook that contains Data Mining Lab Final Exam Cheat Sheet\n",
    "\n",
    "# Where possible, There will be alternate implementations of the same code.\n",
    "# So komal try to use the second method, as I will use the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fp-Growth and Apriori from scratch are not included\n",
    "\n",
    "# From Scratch Only:\n",
    "# 1. KNN and Naive Bayes  For classification\n",
    "# 2. K-Means and Agglomerative for clustering\n",
    "\n",
    "# As for the other algos like Decision Trees, SVM, DBSCAN, BIRCH. You will be allowed to use libraries and knowing them from scratch isnt necessary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is some basic dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('automobile_data.csv')\n",
    "df.head() # to see the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # to see the last 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # This code describes the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # This code prints information about the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # This code prints the column names of the dataframe df.\n",
    "# do remember that df.columns gives an array of column names, which can be then iterated over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique() # This code prints the number of unique values in each column of the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # This code prints the number of null values in each column of the dataframe df."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Null Values (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=df.columns,inplace=False).head() # This code drops all the rows with null values in any column of the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_na = df.dropna() # This code drops all the rows with null values and stores the new dataframe in dropped_df."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imputing null values (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer # This code imports the SimpleImputer class from the sklearn.impute library.\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') # This code creates an object of the SimpleImputer class.\n",
    "\n",
    "# temp = imputer.fit(df[['normalized-losses']]) # This code fits the imputer object on the normalized-losses column of the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing with pandas\n",
    "\n",
    "# temp = df['normalized-losses'].fillna(df['normalized-losses'].mean()) # This code imputes the missing values in the normalized-losses column of the dataframe df with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing with linear regression\n",
    "\n",
    "# temp = df['normalized-losses'].interpolate(method='linear') # This code imputes the missing values in the normalized-losses column of the dataframe df with the linear interpolation of the column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Columns (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach is to choose the columns other than the one to be dropped\n",
    "df[['company', 'body-style', 'wheel-base', 'length', 'num-of-cylinders', 'horsepower', 'price']].head()\n",
    "# here engine-type and average mileage are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method simply drops the columns\n",
    "\n",
    "# when dropping multiple columns, we give an array to the drop function\n",
    "df.drop(['engine-type','average-mileage'], axis=1, inplace=False).head() # axis=1 means column, inplace=True means the changes are made in the original dataframe\n",
    "\n",
    "# dropping a single column\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['column'] == condition] # This code prints all the rows of the dataframe df where the column is equal to condition.\n",
    "df[df['price'] == df['price'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['company'] == 'toyota'] # This code prints all the rows of the dataframe df where the company is equal to toyota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['column'].value_counts() # This code prints the number of times each value occurs in the column of the dataframe df.\n",
    "\n",
    "df['company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='price', ascending=False).head() # This code sorts the dataframe df by price in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=False).head() # This code resets the index of the dataframe df, inplace=False means the changes are not made in the original dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "# df['company'] = le.fit_transform(df['company'])\n",
    "temp = le.fit_transform(df['company']) # This code encodes the company column of the dataframe df.\n",
    "\n",
    "# we can also apply any conditions on columns to only encode categorical values\n",
    "if df['company'].dtype == 'object':\n",
    "    temp = le.fit_transform(df['company'])\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if i not in ['price','horsepower','wheel-base','length','horsepower','average-mileage']:\n",
    "            df[i] = le.fit_transform(df[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['column'].astype('type') # This code changes the type of the column of the dataframe df to type.\n",
    "\n",
    "# another method to encode categorical values\n",
    "# df['company'] = df['company'].astype('category')\n",
    "df['company'].astype('category').cat.codes\n",
    "# we can covert to array by adding .values at the end\n",
    "df['company'].astype('category').cat.codes.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min-max scaling (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# df['column'] = scaler.fit_transform(df['column'])\n",
    "temp = scaler.fit_transform(df[['price']]) # This code scales the price column of the dataframe df.\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively we can scale with numpy\n",
    "# df['column'] = (df['column'] - df['column'].min()) / (df['column'].max() - df['column'].min())\n",
    "temp = (df['price'] - df['price'].min()) / (df['price'].max() - df['price'].min())\n",
    "\n",
    "# convert to array\n",
    "temp.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standard scaling (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['column'] = np.log(df['column']) # This code takes the log of the column of the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# df['column'] = scaler.fit_transform(df['column'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(df.corr(), annot=True) # This code plots the heatmap of the correlation matrix of the dataframe df. annot=True means the values are printed on the heatmap. df.corr() gives the correlation matrix of the dataframe df.\n",
    "\n",
    "# using this we can see which columns are highly correlated and which are not, we can then drop the columns which are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting heatmap with matplotlib, displaying the values\n",
    "plt.imshow(df.corr())\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(df.columns)), df.columns, rotation=90)\n",
    "plt.yticks(range(len(df.columns)), df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "df.corr() # This code prints the correlation matrix of the dataframe df."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')\n",
    "\n",
    "# defining the column names\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# replacing the column names\n",
    "df.columns = columns\n",
    "\n",
    "df.drop(['fnlwgt', 'education-num','relationship','capital-gain','capital-loss'], axis=1, inplace=True)\n",
    "\n",
    "for column in df.columns:\n",
    "    if column not in ['age','hours-per-week']:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual method for train test split\n",
    "\n",
    "# finding the split index\n",
    "split_index = int(0.8 * len(df))\n",
    "\n",
    "# splitting the data\n",
    "train_data = df[:split_index]\n",
    "test_data = df[split_index:]\n",
    "\n",
    "# splitting the data into X and y\n",
    "X_train = train_data.drop('income', axis=1)\n",
    "y_train = train_data['income']\n",
    "\n",
    "X_test = test_data.drop('income', axis=1)\n",
    "y_test = test_data['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # This code splits the dataframe df into train and test sets with 80% of the data in train set and 20% of the data in test set. random_state=42 means the data is split in the same way everytime the code is run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = nb.predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = knn.predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = dt.predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = svm.predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "kmeans.fit(X_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = kmeans.predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Medoiods\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=3)\n",
    "\n",
    "kmedoids.fit(X_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = kmedoids.predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "\n",
    "agg.fit(X_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = agg.fit_predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendrogram\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(X_train, 'ward')\n",
    "\n",
    "dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45, leaf_font_size=15, show_contracted=True)\n",
    "\n",
    "plt.title('Truncated Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "dbscan.fit(X_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = dbscan.fit_predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIRCH\n",
    "\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch = Birch(n_clusters=3)\n",
    "\n",
    "birch.fit(X_train) # This code fits the model on the training data.\n",
    "\n",
    "y_pred = birch.fit_predict(X_test) # This code predicts the target values on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred) # This code prints the accuracy of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternate to print accuracy score is to use model's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate method for printing accuracy score\n",
    "# komal tum yehi use karna\n",
    "\n",
    "print(f'Accuracy score of model is {knn.score(X_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Implementation of KNN (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing KNN manually\n",
    "def knn(X_train, y_train, X_test, y_test, k):\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        dist = []\n",
    "        for j in range(len(X_train)):\n",
    "            dist.append(np.sqrt(np.sum((X_test.iloc[i] - X_train.iloc[j])**2)))\n",
    "        dist = np.array(dist)\n",
    "        dist = np.argsort(dist)[:k]\n",
    "        y_pred.append(y_train.iloc[dist].mode()[0])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "def knn(X_train, y_train, X_test, y_test, k):\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        dist = []\n",
    "        for j in range(len(X_train)):\n",
    "            dist.append(euclidean_distance(X_test.iloc[i], X_train.iloc[j]))\n",
    "        dist = np.array(dist)\n",
    "        dist = np.argsort(dist)[:k]\n",
    "        y_pred.append(y_train.iloc[dist].mode()[0])\n",
    "    return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes (alternates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing naive bayes manually\n",
    "def naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        p = []\n",
    "        for j in np.unique(y_train):\n",
    "            p.append(np.prod(np.exp(-(X_test.iloc[i] - X_train[y_train == j].mean())**2/(2*X_train[y_train == j].std()**2))/(np.sqrt(2*np.pi)*X_train[y_train == j].std())))\n",
    "        y_pred.append(np.unique(y_train)[np.argmax(p)])\n",
    "    return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-mean Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Randomly select k points from the dataset which will be the initial centroids\n",
    "# randomly selecting 3 points from the dataset\n",
    "centroids = {\n",
    "    i+1: [np.random.randint(0, 80), np.random.randint(0, 80)]\n",
    "    for i in range(3)\n",
    "}\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "\n",
    "# Step 2: Assign all the points to the closest cluster centroid\n",
    "# assigning the points to the closest cluster centroid\n",
    "def assignment(data, centroids):\n",
    "    for i in centroids.keys():\n",
    "        # sqrt((x1 - x2)^2 - (y1 - y2)^2)\n",
    "        data['distance_from_{}'.format(i)] = (\n",
    "            np.sqrt(\n",
    "                (data['Age'] - centroids[i][0]) ** 2\n",
    "                + (data['Annual Income (k$)'] - centroids[i][1]) ** 2\n",
    "            )\n",
    "        )\n",
    "    centroid_distance_cols = ['distance_from_{}'.format(i) for i in centroids.keys()]\n",
    "    data['closest'] = data.loc[:, centroid_distance_cols].idxmin(axis=1)\n",
    "    data['closest'] = data['closest'].map(lambda x: int(x.lstrip('distance_from_')))\n",
    "    data['color'] = data['closest'].map(lambda x: colmap[x])\n",
    "    return data\n",
    "\n",
    "data = assignment(data, centroids)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Recompute the centroids of newly formed clusters\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data['Age'], data['Annual Income (k$)'], color=data['color'], alpha=0.5, edgecolor='k')\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Repeat Step 2 and 3\n",
    "import copy\n",
    "\n",
    "old_centroids = copy.deepcopy(centroids)\n",
    "\n",
    "def update(k):\n",
    "    for i in centroids.keys():\n",
    "        centroids[i][0] = np.mean(data[data['closest'] == i]['Age'])\n",
    "        centroids[i][1] = np.mean(data[data['closest'] == i]['Annual Income (k$)'])\n",
    "    return k\n",
    "\n",
    "centroids = update(centroids)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "plt.scatter(data['Age'], data['Annual Income (k$)'], color=data['color'], alpha=0.5, edgecolor='k')\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate distance between two data points\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "# Function to perform agglomerative clustering with single linkage\n",
    "def agglomerative_clustering_single_linkage(xtrain, ytrain, xtest):\n",
    "    num_samples, num_features = xtrain.shape\n",
    "    num_test_samples = xtest.shape[0]\n",
    "    \n",
    "    # Initialize cluster labels for each sample\n",
    "    cluster_labels = np.zeros(num_samples)\n",
    "    \n",
    "    # Calculate distance matrix\n",
    "    distance_matrix = np.zeros((num_samples, num_samples))\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):\n",
    "            distance_matrix[i, j] = calculate_distance(xtrain[i], xtrain[j])\n",
    "            distance_matrix[j, i] = distance_matrix[i, j]\n",
    "    \n",
    "    # Merge clusters until the desired number of clusters is achieved\n",
    "    num_clusters = len(set(ytrain))\n",
    "    while len(set(cluster_labels)) > num_clusters:\n",
    "        # Find the indices of the two closest clusters\n",
    "        min_distance = np.inf\n",
    "        merge_indices = ()\n",
    "        for i in range(num_samples):\n",
    "            for j in range(i+1, num_samples):\n",
    "                if cluster_labels[i] != cluster_labels[j]:\n",
    "                    distance = distance_matrix[i, j]\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        merge_indices = (i, j)\n",
    "        \n",
    "        # Merge the two closest clusters\n",
    "        cluster_labels[merge_indices[1]] = cluster_labels[merge_indices[0]]\n",
    "        \n",
    "    # Assign cluster labels to the test data\n",
    "    test_cluster_labels = np.zeros(num_test_samples)\n",
    "    for i in range(num_test_samples):\n",
    "        distances = np.zeros(num_samples)\n",
    "        for j in range(num_samples):\n",
    "            distances[j] = calculate_distance(xtest[i], xtrain[j])\n",
    "        closest_cluster = cluster_labels[np.argmin(distances)]\n",
    "        test_cluster_labels[i] = closest_cluster\n",
    "    \n",
    "    return test_cluster_labels\n",
    "\n",
    "# Example usage\n",
    "test_cluster_labels = agglomerative_clustering_single_linkage(xtrain, ytrain, xtest)\n",
    "print(\"Test Cluster Labels:\", test_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate distance between two data points\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "# Function to calculate distance matrix\n",
    "def calculate_distance_matrix(xtrain, ytrain):\n",
    "    num_samples, num_features = xtrain.shape\n",
    "    distance_matrix = np.zeros((num_samples, num_samples))\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):\n",
    "            distance_matrix[i, j] = calculate_distance(xtrain[i], xtrain[j])\n",
    "            distance_matrix[j, i] = distance_matrix[i, j]\n",
    "    return distance_matrix\n",
    "\n",
    "# Function to merge clusters\n",
    "def merge_clusters(distance_matrix, cluster_labels):\n",
    "    num_samples = distance_matrix.shape[0]\n",
    "    min_distance = np.inf\n",
    "    merge_indices = ()\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):\n",
    "            if cluster_labels[i] != cluster_labels[j]:\n",
    "                distance = distance_matrix[i, j]\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    merge_indices = (i, j)\n",
    "    cluster_labels[merge_indices[1]] = cluster_labels[merge_indices[0]]\n",
    "    return cluster_labels\n",
    "\n",
    "# Function to perform agglomerative clustering with single linkage\n",
    "def agglomerative_clustering_single_linkage(xtrain, ytrain, xtest):\n",
    "    num_samples, num_features = xtrain.shape\n",
    "    num_test_samples = xtest.shape[0]\n",
    "    \n",
    "    # Initialize cluster labels for each sample\n",
    "    cluster_labels = np.zeros(num_samples)\n",
    "    \n",
    "    # Calculate distance matrix\n",
    "    distance_matrix = calculate_distance_matrix(xtrain, ytrain)\n",
    "    \n",
    "    # Merge clusters until the desired number of clusters is achieved\n",
    "    num_clusters = len(set(ytrain))\n",
    "    while len(set(cluster_labels)) > num_clusters:\n",
    "        # Merge the two closest clusters\n",
    "        cluster_labels = merge_clusters(distance_matrix, cluster_labels)\n",
    "        \n",
    "    # Assign cluster labels to the test data\n",
    "    test_cluster_labels = np.zeros(num_test_samples)\n",
    "    for i in range(num_test_samples):\n",
    "        distances = np.zeros(num_samples)\n",
    "        for j in range(num_samples):\n",
    "            distances[j] = calculate_distance(xtest[i], xtrain[j])\n",
    "        closest_cluster = cluster_labels[np.argmin(distances)]\n",
    "        test_cluster_labels[i] = closest_cluster\n",
    "    \n",
    "    return test_cluster_labels\n",
    "\n",
    "# Example usage\n",
    "test_cluster_labels = agglomerative_clustering_single_linkage(xtrain, ytrain, xtest)\n",
    "print(\"Test Cluster Labels:\", test_cluster_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for elbow curve\n",
    "\n",
    "# elbow curve\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Curve')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
