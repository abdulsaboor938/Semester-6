{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEJl4qh6g1I0",
        "outputId": "89549d93-abab-4863-b920-087508da2b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugiad1T50CQU"
      },
      "source": [
        "**Part:1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z31n5Q1onRJV"
      },
      "outputs": [],
      "source": [
        "# importing required libraries\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import zipfile\n",
        "import collections\n",
        "import pandas as pd\n",
        "\n",
        "# function to read the stopwords file\n",
        "def read_stopwords():\n",
        "    stopwords = {}\n",
        "    with open('Urdu stopwords.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            stopwords[line.strip()] = 1\n",
        "    stopwords[''] = 1 # to remove empty strings\n",
        "    return stopwords\n",
        "\n",
        "# getting max file number in the directory\n",
        "def get_max_file_number():\n",
        "    max_file_number = 0\n",
        "    for file in os.listdir('Documents'):\n",
        "        if file.endswith('.txt'):\n",
        "            try:\n",
        "                file_number = int(file.split('.')[0])\n",
        "            except ValueError:\n",
        "                continue\n",
        "            if file_number > max_file_number:\n",
        "                max_file_number = file_number\n",
        "    return max_file_number\n",
        "\n",
        "\n",
        "# reading a document and doing following:\n",
        "# 1. removing punctuations\n",
        "# 2. removing stopwords\n",
        "# 3. stemming\n",
        "# maintaining a dictionary of {term: term_id}\n",
        "def read_document(file_number, stopwords, term_id):\n",
        "    hash = {}\n",
        "    file_name = 'Documents/' + str(file_number) + '.txt'\n",
        "    with open(file_name, 'r') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'[^ء-ی]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.split(' ')\n",
        "        #text = [word for word in text if word not in stopwords]\n",
        "        #st = ISRIStemmer()\n",
        "        #text = [st.stem(word) for word in text]\n",
        "        count = 0\n",
        "        for word in text:\n",
        "            if word not in term_id:\n",
        "                term_id[word] = len(term_id)\n",
        "            # creating a dictionary of {term_id: [positions]}\n",
        "            if term_id[word] not in hash:\n",
        "                hash[term_id[word]] = [count]\n",
        "            else:\n",
        "                hash[term_id[word]].append(count)\n",
        "            count += 1\n",
        "    return hash\n",
        "\n",
        "# creating an overall hash\n",
        "def create_overall_hash():\n",
        "    hash = {}\n",
        "    max_file_number = get_max_file_number()\n",
        "    stopwords = read_stopwords()\n",
        "    term_id = {}\n",
        "    for file_number in range(1, max_file_number + 1):\n",
        "        try:\n",
        "            hash[file_number] = read_document(file_number, stopwords, term_id)\n",
        "        except:\n",
        "            pass\n",
        "    return hash, term_id\n",
        "\n",
        "# function to write to termids.txt\n",
        "def write_termids(term_id):\n",
        "    with open('termids.txt', 'w') as f:\n",
        "        for key, value in term_id.items():\n",
        "            f.write(f\"{value}\\t{key}\\n\")\n",
        "\n",
        "\n",
        "# function to write to docids.txt\n",
        "def write_docids(hash):\n",
        "    with open('docids.txt', 'w') as f:\n",
        "        count = 1\n",
        "        for key in hash.keys():\n",
        "            f.write(f\"{key}\\t{count}\\n\")\n",
        "            count += 1\n",
        "\n",
        "# function to write to doc_index.txt\n",
        "def write_doc_index(hash):\n",
        "    with open('doc_index.txt','w') as f:\n",
        "        for docid,val in hash.items():\n",
        "            for key, value in val.items():\n",
        "                f.write(f\"{docid}\\t{key}\\t\")\n",
        "                for i in value:\n",
        "                    f.write(f\"{i}\\t\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hash, terms = create_overall_hash()\n",
        "    write_doc_index(hash)\n",
        "    write_docids(hash)\n",
        "    write_termids(terms)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EIGK43zz68D"
      },
      "source": [
        "**Part:2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "InSpw5DW3-qQ"
      },
      "outputs": [],
      "source": [
        "# function to write to term_index.txt\n",
        "def write_term_index(hash):\n",
        "    with open('term_index.txt', 'w') as f:\n",
        "        for termid, val in hash.items():\n",
        "            f.write(f\"{termid}\\t\")\n",
        "            for key, value in val.items():\n",
        "                f.write(f\"{key}:\")\n",
        "                f.write(f\"{value[0]}\\t\")\n",
        "                for i in range(1, len(value)):\n",
        "                    f.write(f\"0:{value[i] - value[i - 1]}\\t\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    write_term_index(hash)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z16_qgvr6EEA"
      },
      "outputs": [],
      "source": [
        "def write_term_info(hash):\n",
        "    with open('term_info.txt', 'w') as f:\n",
        "        for termid, val in hash.items():\n",
        "            f.write(f\"{termid}\\t\")\n",
        "            total_occurrences = 0\n",
        "            total_documents = 0\n",
        "            for key, value in val.items():\n",
        "                total_documents += 1\n",
        "                total_occurrences += len(value)\n",
        "            f.write(f\"{total_occurrences}\\t{total_documents}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    write_term_info(hash)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvFi0JpS0LH6"
      },
      "source": [
        "**Part:3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "WkCxd5N4utJV",
        "outputId": "03e89cd6-9881-44c3-acd2-50161d69b985"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--doc DOC] [--term TERM]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-c79e8df0-28bd-4673-a497-041add5d21f7.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "# function to read term_info.txt\n",
        "def read_term_info():\n",
        "    hash = {}\n",
        "    with open('term_info.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.split('\\t')\n",
        "            hash[int(line[0])] = [int(line[1]), int(line[2])]\n",
        "    return hash\n",
        "\n",
        "\n",
        "# function to read docids.txt\n",
        "def read_docids():\n",
        "    hash = {}\n",
        "    with open('docids.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.split('\\t')\n",
        "            hash[int(line[1])] = int(line[0])\n",
        "    return hash\n",
        "\n",
        "\n",
        "# function to read termids.txt\n",
        "\n",
        "def read_termids():\n",
        "    hash = {}\n",
        "    with open('termids.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.split('\\t')\n",
        "            hash[line[1].strip()] = int(line[0])\n",
        "    return hash\n",
        "\n",
        "\n",
        "# function to read term_index.txt\n",
        "def read_term_index():\n",
        "    hash = {}\n",
        "    with open('term_index.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.split('\\t')\n",
        "            termid = int(line[0])\n",
        "            hash[termid] = {}\n",
        "            for i in range(1, len(line)):\n",
        "                line[i] = line[i].split(':')\n",
        "                docid = int(line[i][0])\n",
        "                hash[termid][docid] = []\n",
        "                line[i] = line[i][1].split(',')\n",
        "                for j in range(len(line[i])):\n",
        "                    hash[termid][docid].append(int(line[i][j]))\n",
        "    return hash\n",
        "\n",
        "\n",
        "# function to read doc_index.txt\n",
        "def read_doc_index():\n",
        "    hash = {}\n",
        "    with open('doc_index.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.split('\\t')\n",
        "            docid = int(line[0])\n",
        "            termid = int(line[1])\n",
        "            hash[docid] = {}\n",
        "            hash[docid][termid] = []\n",
        "            for i in range(2, len(line)):\n",
        "                hash[docid][termid].append(int(line[i]))\n",
        "    return hash\n",
        "\n",
        "\n",
        "\n",
        "# function to get the termid\n",
        "def get_termid(term):\n",
        "    hash = read_termids()\n",
        "    if term in hash:\n",
        "        return hash[term]\n",
        "    else:\n",
        "        return -1\n",
        "    \n",
        "\n",
        "# function to get the docid\n",
        "def get_docid(doc):\n",
        "    hash = read_docids()\n",
        "    if doc in hash:\n",
        "        return hash[doc]\n",
        "    else:\n",
        "        return -1\n",
        "    \n",
        "\n",
        "# function to get the term frequency in corpus\n",
        "def get_term_frequency(termid):\n",
        "    hash = read_term_info()\n",
        "    return hash[termid][0]\n",
        "\n",
        "\n",
        "# function to get the number of documents containing term\n",
        "def get_num_docs(termid):\n",
        "    hash = read_term_info()\n",
        "    return hash[termid][1]\n",
        "\n",
        "\n",
        "# function to get the inverted list offset\n",
        "def get_inverted_list_offset(termid):\n",
        "    hash = read_term_index()\n",
        "    return hash[termid]\n",
        "\n",
        "\n",
        "# function to get the term frequency in document\n",
        "def get_term_frequency_in_doc(termid, docid):\n",
        "    hash = read_doc_index()\n",
        "    return len(hash[docid][termid])\n",
        "\n",
        "\n",
        "# function to get the positions\n",
        "def get_positions(termid, docid):\n",
        "    hash = read_doc_index()\n",
        "    return hash[docid][termid]\n",
        "\n",
        "\n",
        "# function to get the distinct terms\n",
        "def get_distinct_terms(docid):\n",
        "    hash = read_doc_index()\n",
        "    return len(hash[docid])\n",
        "\n",
        "\n",
        "# function to get the total terms\n",
        "def get_total_terms(docid):\n",
        "    hash = read_doc_index()\n",
        "    total = 0\n",
        "    for termid in hash[docid]:\n",
        "        total += len(hash[docid][termid])\n",
        "    return total\n",
        "\n",
        "#write main function\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--doc', type=str, help='doc name')\n",
        "    parser.add_argument('--term', type=str, help='term')\n",
        "    args = parser.parse_args()\n",
        "    if args.doc and args.term:\n",
        "        termid = get_termid(args.term)\n",
        "        docid = get_docid(args.doc)\n",
        "        if termid == -1 or docid == -1:\n",
        "            print('Invalid term or doc')\n",
        "            return\n",
        "        print('Inverted list for term:', args.term)\n",
        "        print('In document:', args.doc)\n",
        "        print('TERMID:', termid)\n",
        "        print('DOCID:', docid)\n",
        "        print('Term frequency in document:', get_term_frequency_in_doc(termid, docid))\n",
        "        print('Positions:', get_positions(termid, docid))\n",
        "    elif args.doc:\n",
        "        docid = get_docid(args.doc)\n",
        "        if docid == -1:\n",
        "            print('Invalid doc')\n",
        "            return\n",
        "        print('Listing for document:', args.doc)\n",
        "        print('DOCID:', docid)\n",
        "        print('Distinct terms:', get_distinct_terms(docid))\n",
        "        print('Total terms:', get_total_terms(docid))\n",
        "    elif args.term:\n",
        "        termid = get_termid(args.term)\n",
        "        if termid == -1:\n",
        "            print('Invalid term')\n",
        "            return\n",
        "        print('Listing for term:', args.term)\n",
        "        print('TERMID:', termid)\n",
        "        print('Number of documents containing term:', get_num_docs(termid))\n",
        "        print('Term frequency in corpus:', get_term_frequency(termid))\n",
        "        print('Inverted list offset:', get_inverted_list_offset(termid))\n",
        "    else:\n",
        "        print('Invalid input')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
